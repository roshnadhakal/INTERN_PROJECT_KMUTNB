{
  "vectorization_methods": [
    "tfidf",
    "tokenizer",
    "sequences",
    "padded_sequences",
    "embedding_matrix",
    "embedding_info"
  ],
  "vocab_size": 70,
  "max_len": 100,
  "timestamp": "2025-07-21T11:58:15.848087",
  "artifacts": {
    "tokenizer": "xss_tokenizer.pkl",
    "sequences": "xss_sequences.npy",
    "tfidf": "xss_tfidf.pkl",
    "bow": null,
    "embedding_matrix": "embedding_matrix.npy"
  }
}